{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding t-SNE visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ben Greenawald (bhg5yd)\n",
    "Based on https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgree\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "import os\n",
    "import progressbar\n",
    "import pickle\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to clean sentences\n",
    "STOP_WORDS = nltk.corpus.stopwords.words()\n",
    "\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)  \n",
    "            \n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 5502 Elapsed Time: 0:10:16                                                   \n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "data_path = \"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\eng_clean\\\\\"\n",
    "\n",
    "# Read in all files\n",
    "sentences = [0] * len(os.listdir(data_path)) \n",
    "files = [0] * len(os.listdir(data_path)) \n",
    "bar = progressbar.ProgressBar()\n",
    "\n",
    "for i, file in bar(enumerate(os.listdir(data_path))):\n",
    "    with open((data_path + file), \"r\") as cur_file:\n",
    "        sentences[i] = clean_sentence(cur_file.read())\n",
    "        files[i] = file\n",
    "        cur_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace newlines with spaces, replace multiple spaces with single space\n",
    "newline = re.compile(\"[\\n]+\")\n",
    "multispace = re.compile(\"[ ]+\")\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence = newline.sub(' ', sentence)\n",
    "    sentence = multispace.sub(' ', sentence)\n",
    "    sentences[i] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle the list so we don't have to read it in again\n",
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)\n",
    "    f.close()\n",
    "    \n",
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\files.pkl', 'wb') as f:\n",
    "    pickle.dump(files, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\files.pkl', 'rb') as f:\n",
    "    files = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = [0] * len(data)\n",
    "    for i, sentence in enumerate(data):\n",
    "        word_list = sentence.split(\" \")\n",
    "        corpus[i] = word_list\n",
    "            \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = build_corpus(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(corpus, size=100, window=5, min_count=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feed', 0.4436623752117157),\n",
       " ('save', 0.42832493782043457),\n",
       " ('physically', 0.38688328862190247),\n",
       " ('death', 0.382870078086853),\n",
       " ('drown', 0.371822327375412),\n",
       " ('heal', 0.36274629831314087),\n",
       " ('restore', 0.359414279460907),\n",
       " ('recover', 0.35854101181030273),\n",
       " ('starve', 0.35682594776153564),\n",
       " ('killed', 0.35319411754608154)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['kill', 'love'], negative=['hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nurse', 0.6744319200515747),\n",
       " ('hospital', 0.6417989134788513),\n",
       " ('patients', 0.6167763471603394),\n",
       " ('doctors', 0.5895438194274902),\n",
       " ('clinic', 0.5791116952896118),\n",
       " ('shane', 0.5531526803970337),\n",
       " ('mom', 0.5406304597854614),\n",
       " ('sick', 0.5246722102165222),\n",
       " ('surgery', 0.5209794044494629),\n",
       " ('teenager', 0.5186388492584229)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['doctor', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wife', 0.770444393157959),\n",
       " ('wives', 0.5845280289649963),\n",
       " ('zainab', 0.5403890013694763),\n",
       " ('husbands', 0.539920449256897),\n",
       " ('marriage', 0.5272690057754517),\n",
       " ('daughter', 0.5203981399536133),\n",
       " ('women', 0.5176483392715454),\n",
       " ('divorced', 0.4951685070991516),\n",
       " ('nurse', 0.49472132325172424),\n",
       " ('unmarried', 0.48751023411750793)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['husband', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doomed', 0.4933154582977295),\n",
       " ('nation', 0.47456789016723633),\n",
       " ('patriotic', 0.46974021196365356),\n",
       " ('continent', 0.45280495285987854),\n",
       " ('civilization', 0.43584635853767395),\n",
       " ('democracy', 0.4354703724384308),\n",
       " ('europe', 0.4346542954444885),\n",
       " ('anglo', 0.42719873785972595),\n",
       " ('liberalism', 0.4231260418891907),\n",
       " ('eastern', 0.41540855169296265)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['america'], negative=['money'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pakistan', 0.5671586990356445),\n",
       " ('sweden', 0.5523396730422974),\n",
       " ('indian', 0.5497785806655884),\n",
       " ('korea', 0.5490356683731079),\n",
       " ('african', 0.5363327264785767),\n",
       " ('negroes', 0.5339013338088989),\n",
       " ('africans', 0.5332790613174438),\n",
       " ('indonesia', 0.5225752592086792),\n",
       " ('usa', 0.5220274925231934),\n",
       " ('korean', 0.5138765573501587)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['america'], negative=['hope'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(n_components=2, random_state=23, verbose=3)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    d = {\"x\": x, \n",
    "         \"y\":y,\n",
    "         \"words\": labels}\n",
    "    \n",
    "    return pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgree\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 12050 samples in 0.053s...\n",
      "[t-SNE] Computed neighbors for 12050 samples in 30.726s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 12050\n",
      "[t-SNE] Computed conditional probabilities for sample 12050 / 12050\n",
      "[t-SNE] Mean sigma: 0.840219\n",
      "[t-SNE] Computed conditional probabilities in 0.454s\n",
      "[t-SNE] Iteration 50: error = 95.0885773, gradient norm = 0.0373409 (50 iterations in 28.079s)\n",
      "[t-SNE] Iteration 100: error = 95.2065125, gradient norm = 0.0108383 (50 iterations in 26.733s)\n",
      "[t-SNE] Iteration 150: error = 95.0272827, gradient norm = 0.0085695 (50 iterations in 25.674s)\n",
      "[t-SNE] Iteration 200: error = 95.0035629, gradient norm = 0.0427824 (50 iterations in 22.085s)\n",
      "[t-SNE] Iteration 250: error = 94.7399826, gradient norm = 0.0274619 (50 iterations in 22.349s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 94.739983\n",
      "[t-SNE] Iteration 300: error = 3.7992411, gradient norm = 0.0010491 (50 iterations in 22.745s)\n",
      "[t-SNE] Iteration 350: error = 3.4868751, gradient norm = 0.0004958 (50 iterations in 20.981s)\n",
      "[t-SNE] Iteration 400: error = 3.3391721, gradient norm = 0.0002962 (50 iterations in 21.097s)\n",
      "[t-SNE] Iteration 450: error = 3.2469645, gradient norm = 0.0002031 (50 iterations in 20.942s)\n",
      "[t-SNE] Iteration 500: error = 3.1837416, gradient norm = 0.0001507 (50 iterations in 20.739s)\n",
      "[t-SNE] Iteration 550: error = 3.1376946, gradient norm = 0.0001189 (50 iterations in 20.684s)\n",
      "[t-SNE] Iteration 600: error = 3.1019511, gradient norm = 0.0000989 (50 iterations in 20.834s)\n",
      "[t-SNE] Iteration 650: error = 3.0739489, gradient norm = 0.0000832 (50 iterations in 20.912s)\n",
      "[t-SNE] Iteration 700: error = 3.0515647, gradient norm = 0.0000730 (50 iterations in 20.867s)\n",
      "[t-SNE] Iteration 750: error = 3.0331788, gradient norm = 0.0000623 (50 iterations in 20.903s)\n",
      "[t-SNE] Iteration 800: error = 3.0179467, gradient norm = 0.0000548 (50 iterations in 20.877s)\n",
      "[t-SNE] Iteration 850: error = 3.0050015, gradient norm = 0.0000497 (50 iterations in 21.484s)\n",
      "[t-SNE] Iteration 900: error = 2.9941509, gradient norm = 0.0000445 (50 iterations in 20.479s)\n",
      "[t-SNE] Iteration 950: error = 2.9848399, gradient norm = 0.0000413 (50 iterations in 20.628s)\n",
      "[t-SNE] Iteration 1000: error = 2.9770401, gradient norm = 0.0000407 (50 iterations in 21.066s)\n",
      "[t-SNE] Error after 1000 iterations: 2.977040\n"
     ]
    }
   ],
   "source": [
    "tsne_data = tsne_plot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_data.to_csv(\"wordTSNE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the document labels\n",
    "group_to_labels = {}\n",
    "with open(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\eng_group_labels.txt\", \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        line_split = line.split(\",\")\n",
    "        group_to_labels[line_split[0]] = int(line_split[1])\n",
    "        \n",
    "# Get the filenames and labels\n",
    "digit_remover = re.compile(\"[\\d]+\")\n",
    "\n",
    "groups_labels = [0] * len(sentences)\n",
    "binary_labels = [0] * len(sentences)\n",
    "for i, file in enumerate(files):\n",
    "    f = file.split('.')[0]\n",
    "    f = digit_remover.sub('',f)\n",
    "    if f[-1] == '-':\n",
    "        f = f[:-1]\n",
    "    if f == 'AndrewMurray-HolyinChrist':\n",
    "        f = 'AndrewMurray'\n",
    "    groups_labels[i] = f\n",
    "    binary_labels[i] = group_to_labels[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [0] * len(sentences)\n",
    "for i, doc in enumerate(sentences):\n",
    "     str_list = doc.split()\n",
    "     t = doc2vec.TaggedDocument(str_list,[i])\n",
    "     docs[i] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgree\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "modelDoc = doc2vec.Doc2Vec(docs, size=100, window=8, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelDoc.save(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\modelDoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelDoc = word2vec.Word2Vec.load(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Eng\\\\modelDoc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsne_doc(model):\n",
    "    docs = []\n",
    "    for i in range(5503):\n",
    "        docs.append(model.docvecs[i])\n",
    "    \n",
    "    tsne_model = TSNE(n_components=2, random_state=23, verbose=3)\n",
    "    new_values = tsne_model.fit_transform(docs)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    d = {\"x\": x, \n",
    "         \"y\":y\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 5503 samples in 0.026s...\n",
      "[t-SNE] Computed neighbors for 5503 samples in 4.106s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 5503\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 5503\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 5503\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 5503\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 5503\n",
      "[t-SNE] Computed conditional probabilities for sample 5503 / 5503\n",
      "[t-SNE] Mean sigma: 0.663664\n",
      "[t-SNE] Computed conditional probabilities in 0.174s\n",
      "[t-SNE] Iteration 50: error = 85.7676392, gradient norm = 0.0452217 (50 iterations in 12.517s)\n",
      "[t-SNE] Iteration 100: error = 85.8437500, gradient norm = 0.0302953 (50 iterations in 12.008s)\n",
      "[t-SNE] Iteration 150: error = 85.8016357, gradient norm = 0.0296729 (50 iterations in 21.506s)\n",
      "[t-SNE] Iteration 200: error = 85.6228256, gradient norm = 0.0444155 (50 iterations in 16.705s)\n",
      "[t-SNE] Iteration 250: error = 85.8885422, gradient norm = 0.0217182 (50 iterations in 21.924s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 85.888542\n",
      "[t-SNE] Iteration 300: error = 3.1420171, gradient norm = 0.0011750 (50 iterations in 8.788s)\n",
      "[t-SNE] Iteration 350: error = 2.9393091, gradient norm = 0.0004231 (50 iterations in 7.456s)\n",
      "[t-SNE] Iteration 400: error = 2.8492389, gradient norm = 0.0002322 (50 iterations in 7.289s)\n",
      "[t-SNE] Iteration 450: error = 2.7987199, gradient norm = 0.0001576 (50 iterations in 7.367s)\n",
      "[t-SNE] Iteration 500: error = 2.7663517, gradient norm = 0.0001258 (50 iterations in 7.428s)\n",
      "[t-SNE] Iteration 550: error = 2.7449746, gradient norm = 0.0001032 (50 iterations in 7.398s)\n",
      "[t-SNE] Iteration 600: error = 2.7301342, gradient norm = 0.0000828 (50 iterations in 7.362s)\n",
      "[t-SNE] Iteration 650: error = 2.7193618, gradient norm = 0.0000730 (50 iterations in 7.388s)\n",
      "[t-SNE] Iteration 700: error = 2.7119226, gradient norm = 0.0000668 (50 iterations in 7.392s)\n",
      "[t-SNE] Iteration 750: error = 2.7062137, gradient norm = 0.0000568 (50 iterations in 7.366s)\n",
      "[t-SNE] Iteration 800: error = 2.7013857, gradient norm = 0.0000533 (50 iterations in 7.862s)\n",
      "[t-SNE] Iteration 850: error = 2.6975021, gradient norm = 0.0000483 (50 iterations in 8.152s)\n",
      "[t-SNE] Iteration 900: error = 2.6941586, gradient norm = 0.0000478 (50 iterations in 8.685s)\n",
      "[t-SNE] Iteration 950: error = 2.6914542, gradient norm = 0.0000470 (50 iterations in 8.250s)\n",
      "[t-SNE] Iteration 1000: error = 2.6893280, gradient norm = 0.0000584 (50 iterations in 8.069s)\n",
      "[t-SNE] Error after 1000 iterations: 2.689328\n"
     ]
    }
   ],
   "source": [
    "tsne_data2 = tsne_doc(modelDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_data2['binary'] = binary_labels\n",
    "tsne_data2['group'] = groups_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_data2.to_csv(\"docTSNE.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 0 Elapsed Time: 0:00:11                                                      \n",
      "| 14857 Elapsed Time: 0:00:53                                                  \n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "ar_data_path = \"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Models\\\\arabic-docs\\\\\"\n",
    "\n",
    "# Read in all files\n",
    "ar_sentences = [0] * len(os.listdir(ar_data_path)) \n",
    "ar_files = [0] * len(os.listdir(ar_data_path)) \n",
    "bar = progressbar.ProgressBar()\n",
    "\n",
    "for i, file in bar(enumerate(os.listdir(ar_data_path))):\n",
    "    with open((ar_data_path + file), \"r\", encoding='utf-8') as cur_file:\n",
    "        ar_sentences[i] = cur_file.read().lower()\n",
    "        ar_files[i] = file\n",
    "        cur_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace newlines with spaces, replace multiple spaces with single space\n",
    "newline = re.compile(\"[\\n]+\")\n",
    "multispace = re.compile(\"[ ]+\")\n",
    "\n",
    "for i, sentence in enumerate(ar_sentences):\n",
    "    sentence = newline.sub(' ', sentence)\n",
    "    sentence = multispace.sub(' ', sentence)\n",
    "    ar_sentences[i] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle the list so we don't have to read it in again\n",
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(ar_sentences, f)\n",
    "    f.close()\n",
    "    \n",
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\files.pkl', 'wb') as f:\n",
    "    pickle.dump(ar_files, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN JUST THIS BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\sentences.pkl', 'rb') as f:\n",
    "    ar_sentences = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\files.pkl', 'rb') as f:\n",
    "    ar_files = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = [0] * len(data)\n",
    "    for i, sentence in enumerate(data):\n",
    "        word_list = sentence.split(\" \")\n",
    "        corpus[i] = word_list\n",
    "            \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_corpus = build_corpus(ar_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_model = word2vec.Word2Vec(ar_corpus, size=100, window=5, min_count=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_model.save(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN THIS BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_model = word2vec.Word2Vec.load(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add vector similarity operations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('عاش', 0.5300116539001465),\n",
       " ('ترعرع', 0.5264725685119629),\n",
       " ('نشأ', 0.525036096572876),\n",
       " ('تربى', 0.5183085203170776),\n",
       " ('واستشهد', 0.5158625841140747),\n",
       " ('طفولته', 0.5106531977653503),\n",
       " ('رباه', 0.49314388632774353),\n",
       " ('وترعرع', 0.4907999634742737),\n",
       " ('ربى', 0.48872873187065125),\n",
       " ('تأثر', 0.48401132225990295)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kill - evil + love\n",
    "ar_model.wv.most_similar(positive=['قتل', 'حب'], negative=['شر'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('الوالدين', 0.5931550860404968),\n",
       " ('النساء،', 0.5799429416656494),\n",
       " ('عليهن', 0.5544040203094482),\n",
       " ('السيئات', 0.5504500865936279),\n",
       " ('إهمال', 0.5453451871871948),\n",
       " ('الظن', 0.5373238921165466),\n",
       " ('المسلمات', 0.524681806564331),\n",
       " ('الزوجات', 0.5192272663116455),\n",
       " ('الطلاق', 0.5082723498344421),\n",
       " ('بالنساء', 0.503494143486023)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# husband - man + woman\n",
    "ar_model.wv.most_similar(positive=['النساء', 'الزوج'], negative=['رجل'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('الإرهاب', 0.49271541833877563),\n",
       " ('وبريطانيا', 0.47849082946777344),\n",
       " ('الأمريكية', 0.4498194754123688),\n",
       " ('العراق،', 0.44678795337677),\n",
       " ('العصابات', 0.4376118779182434),\n",
       " ('قطر', 0.4372726380825043),\n",
       " ('الواليات', 0.43432122468948364),\n",
       " ('مجازر', 0.4332858920097351),\n",
       " ('ليبيا', 0.42830702662467957),\n",
       " ('بالعراق', 0.42714524269104004)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# america - hope\n",
    "ar_model.wv.most_similar(positive=['أمريكا'], negative=['أمل'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(n_components=2, random_state=23, verbose=3)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    d = {\"x\": x, \n",
    "         \"y\":y,\n",
    "         \"words\": labels}\n",
    "    \n",
    "    return pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgree\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 23378 samples in 0.137s...\n",
      "[t-SNE] Computed neighbors for 23378 samples in 113.351s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 23378\n",
      "[t-SNE] Computed conditional probabilities for sample 23378 / 23378\n",
      "[t-SNE] Mean sigma: 0.795669\n",
      "[t-SNE] Computed conditional probabilities in 0.904s\n",
      "[t-SNE] Iteration 50: error = 102.9278259, gradient norm = 0.0050205 (50 iterations in 61.098s)\n",
      "[t-SNE] Iteration 100: error = 102.9201889, gradient norm = 0.0003764 (50 iterations in 78.106s)\n",
      "[t-SNE] Iteration 150: error = 101.7430038, gradient norm = 0.0002486 (50 iterations in 55.957s)\n",
      "[t-SNE] Iteration 200: error = 101.3917313, gradient norm = 0.0002098 (50 iterations in 69.186s)\n",
      "[t-SNE] Iteration 250: error = 101.3217773, gradient norm = 0.0000194 (50 iterations in 67.459s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 101.321777\n",
      "[t-SNE] Iteration 300: error = 4.4416533, gradient norm = 0.0010727 (50 iterations in 54.720s)\n",
      "[t-SNE] Iteration 350: error = 4.0455570, gradient norm = 0.0005521 (50 iterations in 45.433s)\n",
      "[t-SNE] Iteration 400: error = 3.8353503, gradient norm = 0.0003472 (50 iterations in 44.948s)\n",
      "[t-SNE] Iteration 450: error = 3.7030609, gradient norm = 0.0002454 (50 iterations in 44.717s)\n",
      "[t-SNE] Iteration 500: error = 3.6111128, gradient norm = 0.0001838 (50 iterations in 44.110s)\n",
      "[t-SNE] Iteration 550: error = 3.5410056, gradient norm = 0.0001466 (50 iterations in 43.816s)\n",
      "[t-SNE] Iteration 600: error = 3.4863467, gradient norm = 0.0001208 (50 iterations in 43.962s)\n",
      "[t-SNE] Iteration 650: error = 3.4419076, gradient norm = 0.0001030 (50 iterations in 44.338s)\n",
      "[t-SNE] Iteration 700: error = 3.4036591, gradient norm = 0.0000887 (50 iterations in 42.635s)\n",
      "[t-SNE] Iteration 750: error = 3.3713620, gradient norm = 0.0000784 (50 iterations in 44.192s)\n",
      "[t-SNE] Iteration 800: error = 3.3438203, gradient norm = 0.0000702 (50 iterations in 40.021s)\n",
      "[t-SNE] Iteration 850: error = 3.3201447, gradient norm = 0.0000631 (50 iterations in 37.254s)\n",
      "[t-SNE] Iteration 900: error = 3.2991419, gradient norm = 0.0000569 (50 iterations in 37.267s)\n",
      "[t-SNE] Iteration 950: error = 3.2807055, gradient norm = 0.0000521 (50 iterations in 41.474s)\n",
      "[t-SNE] Iteration 1000: error = 3.2643080, gradient norm = 0.0000476 (50 iterations in 44.373s)\n",
      "[t-SNE] Error after 1000 iterations: 3.264308\n"
     ]
    }
   ],
   "source": [
    "tsne_data = tsne_plot(ar_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle the list so we don't have to read it in again\n",
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\word_tsne.pkl', 'wb') as f:\n",
    "    pickle.dump(tsne_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\word_tsne.pkl', 'rb') as f:\n",
    "    word_tsne = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_data.to_csv(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\wordTSNE.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the document labels\n",
    "group_to_labels = {}\n",
    "with open(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\arabic-groups-labels.txt\", \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        line_split = line.split(\",\")\n",
    "        group_to_labels[line_split[0]] = int(line_split[1])\n",
    "        \n",
    "groups_labels = [0] * len(ar_sentences)\n",
    "binary_labels = [0] * len(ar_sentences)\n",
    "for i, file in enumerate(ar_files):\n",
    "    if '-g-' in file:\n",
    "        f = file.split('-g-')[0]\n",
    "    else:\n",
    "        f = file.split('_g-')[0]    \n",
    "    groups_labels[i] = f\n",
    "    binary_labels[i] = group_to_labels[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_docs = [0] * len(ar_sentences)\n",
    "for i, doc in enumerate(ar_sentences):\n",
    "     str_list = doc.split()\n",
    "     t = doc2vec.TaggedDocument(str_list,[i])\n",
    "     ar_docs[i] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgree\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "ar_modelDoc = doc2vec.Doc2Vec(ar_docs, size=100, window=8, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_modelDoc.save(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\modelDoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_modelDoc = word2vec.Word2Vec.load(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\modelDoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsne_doc(model):\n",
    "    docs = []\n",
    "    for i in range(14858):\n",
    "        docs.append(model.docvecs[i])\n",
    "    \n",
    "    tsne_model = TSNE(n_components=2, random_state=23, verbose=3)\n",
    "    new_values = tsne_model.fit_transform(docs)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    d = {\"x\": x, \n",
    "         \"y\":y\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 14858 samples in 0.088s...\n",
      "[t-SNE] Computed neighbors for 14858 samples in 40.356s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 14858\n",
      "[t-SNE] Computed conditional probabilities for sample 14858 / 14858\n",
      "[t-SNE] Mean sigma: 0.282929\n",
      "[t-SNE] Computed conditional probabilities in 0.535s\n",
      "[t-SNE] Iteration 50: error = 97.2662506, gradient norm = 0.0100723 (50 iterations in 55.956s)\n",
      "[t-SNE] Iteration 100: error = 94.0539017, gradient norm = 0.0047230 (50 iterations in 36.225s)\n",
      "[t-SNE] Iteration 150: error = 93.8709106, gradient norm = 0.0120493 (50 iterations in 32.849s)\n",
      "[t-SNE] Iteration 200: error = 93.8195114, gradient norm = 0.0090953 (50 iterations in 33.802s)\n",
      "[t-SNE] Iteration 250: error = 93.8006973, gradient norm = 0.0039072 (50 iterations in 34.353s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 93.800697\n",
      "[t-SNE] Iteration 300: error = 3.8525648, gradient norm = 0.0009643 (50 iterations in 28.035s)\n",
      "[t-SNE] Iteration 350: error = 3.5676057, gradient norm = 0.0004330 (50 iterations in 24.586s)\n",
      "[t-SNE] Iteration 400: error = 3.4362030, gradient norm = 0.0002653 (50 iterations in 24.537s)\n",
      "[t-SNE] Iteration 450: error = 3.3569076, gradient norm = 0.0001849 (50 iterations in 24.482s)\n",
      "[t-SNE] Iteration 500: error = 3.3005714, gradient norm = 0.0001398 (50 iterations in 24.380s)\n",
      "[t-SNE] Iteration 550: error = 3.2589948, gradient norm = 0.0001098 (50 iterations in 24.531s)\n",
      "[t-SNE] Iteration 600: error = 3.2265522, gradient norm = 0.0000901 (50 iterations in 24.559s)\n",
      "[t-SNE] Iteration 650: error = 3.2007384, gradient norm = 0.0000767 (50 iterations in 24.319s)\n",
      "[t-SNE] Iteration 700: error = 3.1797659, gradient norm = 0.0000654 (50 iterations in 24.312s)\n",
      "[t-SNE] Iteration 750: error = 3.1623280, gradient norm = 0.0000573 (50 iterations in 24.386s)\n",
      "[t-SNE] Iteration 800: error = 3.1474357, gradient norm = 0.0000511 (50 iterations in 24.450s)\n",
      "[t-SNE] Iteration 850: error = 3.1348236, gradient norm = 0.0000457 (50 iterations in 24.650s)\n",
      "[t-SNE] Iteration 900: error = 3.1237936, gradient norm = 0.0000414 (50 iterations in 24.981s)\n",
      "[t-SNE] Iteration 950: error = 3.1143546, gradient norm = 0.0000384 (50 iterations in 24.444s)\n",
      "[t-SNE] Iteration 1000: error = 3.1061792, gradient norm = 0.0000354 (50 iterations in 24.612s)\n",
      "[t-SNE] Error after 1000 iterations: 3.106179\n"
     ]
    }
   ],
   "source": [
    "tsne_data2 = tsne_doc(ar_modelDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_data2['binary'] = binary_labels\n",
    "tsne_data2['group'] = groups_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_data2.to_csv(\"C:\\\\Users\\\\bgree\\\\Documents\\\\capstone\\\\Ar\\\\docTSNE.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
